{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff36039",
   "metadata": {},
   "source": [
    "# Improving ModSecurity WAF Using a Structured-Language Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50d454",
   "metadata": {},
   "source": [
    "## For Computer Science B.Sc. Ariel University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f4593e",
   "metadata": {},
   "source": [
    "By Maor Saadon, Shahar Zaidel, Eden Mor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56246420",
   "metadata": {},
   "source": [
    "# Introdaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936f597",
   "metadata": {},
   "source": [
    "In the ever-evolving landscape of cybersecurity, Web Application Firewalls (WAF) stand as critical defenders against malicious web requests. Traditionally, WAFs rely on predefined rule sets to identify and block potential threats. However, the complexity and sophistication of modern cyber attacks often outpace the capabilities of these rule-based systems, necessitating more dynamic and adaptive approaches. Machine learning (ML), particularly deep learning, has emerged as a promising solution, offering the potential to significantly enhance the detection and mitigation capabilities of WAFs through its ability to learn from data and identify complex patterns.\n",
    "\n",
    "Our research explores the integration of machine learning techniques into the domain of web application security, focusing on the comparative analysis of various ML models in classifying web requests. We delve into models based on advanced text vectorization techniques, such as TF-IDF and Word2Vec, which transform web requests into numerical vectors that capture their semantic essence. These vectors are then utilized by machine learning algorithms, including the traditional k-Nearest Neighbors (kNN) algorithm, to distinguish between benign and malicious requests effectively.\n",
    "\n",
    "Central to our investigation is the exploration of deep learning models in contrast to traditional ML models. Deep learning's capacity for learning hierarchical representations offers a nuanced approach to understanding and classifying web traffic. This leads us to our guiding research question: Does a machine learning model enhanced with deep learning techniques outperform traditional models in the context of Web Application Firewalls?\n",
    "\n",
    "By examining the efficacy of deep learning models against their non-deep learning counterparts, our study aims to shed light on their relative performance and potential for improving the security mechanisms of WAFs. Through this comparative analysis, we aspire to contribute valuable insights into the advancement of cybersecurity methodologies, specifically in enhancing the resilience of web applications against the growing threat of cyber attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f46ca",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d07f0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\maor1\\anaconda3\\lib\\site-packages (2.15.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maor1\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da6b87b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shaha\\PycharmProjects\\CMD_final_project\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import getopt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pickle as pkl\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08339edc",
   "metadata": {},
   "source": [
    "## Feature extraction functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ba8cf",
   "metadata": {},
   "source": [
    "We will use the following feature extraction functions to extract features from the HTTP request:\n",
    "1. calculate_alphanumeric_ratio\n",
    "2. calculate_input_length\n",
    "3. calculate_special_character_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f7a5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alphanumeric_ratio(payload):\n",
    "    alphanumeric_characters = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "    alphanumeric_count = sum(1 for char in payload if char in alphanumeric_characters)\n",
    "    payload_length = len(payload)\n",
    "    input_length = max(payload_length, 1)  # Avoid division by zero if payload_length is 0\n",
    "    alphanumeric_ratio = (alphanumeric_count / input_length) * 10\n",
    "    return alphanumeric_ratio\n",
    "\n",
    "def calculate_input_length(payload):\n",
    "    input_length = len(payload)\n",
    "    return input_length\n",
    "\n",
    "def calculate_special_character_ratio(payload):\n",
    "    alphanumeric_characters = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "    special_count = sum(1 for char in payload if char not in alphanumeric_characters)\n",
    "    payload_length = len(payload)\n",
    "    input_length = max(payload_length, 1)  # Avoid division by zero if payload_length is 0\n",
    "    special_ratio = (special_count / input_length) * 100\n",
    "    return special_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3767a62",
   "metadata": {},
   "source": [
    "# Load and Procces the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b802d",
   "metadata": {},
   "source": [
    "The dataset is a CSV file with two columns: payload and label. The payload column contains the http request payload and the label column contains the label of the http request. The label is 1 if the request is malicious and 0 if the request is benign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61629e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Dataset Shape: (31067, 5)\n",
      "Balanced Dataset Distribution: label\n",
      "0    19304\n",
      "1    11763\n",
      "Name: count, dtype: int64\n",
      "Balanced Dataset Head (First 10 Rows):\n",
      "                                             payload  payload_len       alpha  \\\n",
      "0                                             prueba            6  100.000000   \n",
      "1  -9072%29%20as%20saot%20where%202278%3D2278%20o...           42   90.476190   \n",
      "2                          palma%20de%20cervell%20la           19  100.000000   \n",
      "3                                       6pa439ue494s           12  100.000000   \n",
      "4  1%27%7C%7C%28select%20%27gwxc%27%20from%20dual...          185   74.054054   \n",
      "5                        schwab.carli%40emepleo.info           25  100.000000   \n",
      "6                                          91704360w            9  100.000000   \n",
      "7                  callejon%20sierpes%20128%2C%205-e           25  100.000000   \n",
      "8  -2254%27%29%20as%20dyrv%20where%209079%3D9079%...           89   86.516854   \n",
      "9  1%27%29%29%20as%20zqeg%20where%204729%3D4729%2...           51   80.392157   \n",
      "\n",
      "   non_alpha  label  \n",
      "0   0.000000      0  \n",
      "1   9.523810      1  \n",
      "2   0.000000      0  \n",
      "3   0.000000      0  \n",
      "4  25.945946      1  \n",
      "5   0.000000      0  \n",
      "6   0.000000      0  \n",
      "7   0.000000      0  \n",
      "8  13.483146      1  \n",
      "9  19.607843      1  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('HTTPParams.csv')\n",
    "df = df.drop(['attack_feature'], axis=1)\n",
    "\n",
    "# Shuffle the combined dataset to mix malicious and benign URLs\n",
    "balanced_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print('Balanced Dataset Shape:', balanced_df.shape)\n",
    "print('Balanced Dataset Distribution:', balanced_df['label'].value_counts())\n",
    "print('Balanced Dataset Head (First 10 Rows):')\n",
    "print(balanced_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45757f91",
   "metadata": {},
   "source": [
    "## Start testing with the options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69516ab5",
   "metadata": {},
   "source": [
    "We'll split the truncated dataset into a training set and a test set (80% training, 20% test) and use the training set to train the model and the test set to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b97a4",
   "metadata": {},
   "source": [
    "# W2V Vectorizer preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "573e7006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction...\n",
      "Balanced Dataset Shape: (31067, 8)\n",
      "Balanced Dataset:\n",
      "                                                 payload  payload_len  \\\n",
      "0                                                 prueba            6   \n",
      "1      -9072%29%20as%20saot%20where%202278%3D2278%20o...           42   \n",
      "2                              palma%20de%20cervell%20la           19   \n",
      "3                                           6pa439ue494s           12   \n",
      "4      1%27%7C%7C%28select%20%27gwxc%27%20from%20dual...          185   \n",
      "...                                                  ...          ...   \n",
      "31062  1%22%20and%206510%3D%28select%20count%28%2A%29...          175   \n",
      "31063                                           andruzzi            8   \n",
      "31064                                             sibila            6   \n",
      "31065  -9779%27%29%29%20or%20make_set%289354%3D9354%2...           55   \n",
      "31066                                 depp%40rpdkorea.il           16   \n",
      "\n",
      "            alpha  non_alpha  label  payload_len      alpha  non_alpha  \n",
      "0      100.000000   0.000000      0          6.0  10.000000   0.000000  \n",
      "1       90.476190   9.523810      1         62.0   8.225806  17.741935  \n",
      "2      100.000000   0.000000      0         25.0   8.800000  12.000000  \n",
      "3      100.000000   0.000000      0         12.0  10.000000   0.000000  \n",
      "4       74.054054  25.945946      1        287.0   8.083624  19.163763  \n",
      "...           ...        ...    ...          ...        ...        ...  \n",
      "31062   90.857143   9.142857      1        253.0   8.458498  15.415020  \n",
      "31063  100.000000   0.000000      0          8.0  10.000000   0.000000  \n",
      "31064  100.000000   0.000000      0          6.0  10.000000   0.000000  \n",
      "31065   72.727273  27.272727      1         89.0   7.865169  21.348315  \n",
      "31066  100.000000   0.000000      0         18.0   8.888889  11.111111  \n",
      "\n",
      "[31067 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Apply feature extraction\n",
    "print('Feature extraction...')\n",
    "features = balanced_df['payload'].apply(lambda x: pd.Series({\n",
    "    'payload_len': calculate_input_length(str(x)),\n",
    "    'alpha': calculate_alphanumeric_ratio(str(x)),\n",
    "    'non_alpha': calculate_special_character_ratio(str(x)),\n",
    "    }))\n",
    "\n",
    "# Concatenate original DF with features\n",
    "balanced_df = pd.concat([balanced_df, features], axis=1)\n",
    "print('Balanced Dataset Shape:', balanced_df.shape)\n",
    "print('Balanced Dataset:')\n",
    "print(balanced_df)\n",
    "\n",
    "# Before tokenization, replace NaN values in 'payload'   with empty strings\n",
    "balanced_df['payload'] = balanced_df['payload'].fillna('z')\n",
    "\n",
    "# Tokenize HTTP requests\n",
    "tokenized_requests = [str(request).split('/') for request in balanced_df['payload']]\n",
    "\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(tokenized_requests, vector_size=100, window=4, min_count=1, workers=4)\n",
    "\n",
    "X_combined = np.array([np.mean([model.wv[word] for word in request if word in model.wv], axis=0) for request in tokenized_requests])\n",
    "\n",
    "# Define y\n",
    "y = balanced_df['label'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e7554f",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840aabe4",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b02d59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors training is complete,\n",
      " predictions: \n",
      "[+] \t Classification accuracy :  39.96 %\n",
      "\n",
      "[+] \t Percentage of Anomaly requests in test set :  38.38 %\n",
      "\n",
      "[+] \t Percentage of Normal requests in test set :  61.62 %\n",
      "\n",
      "[+] \t TP :  141 from  6214\n",
      "    \t TN :  2342 from  6214\n",
      "    \t FP :  3688 from  6214\n",
      "    \t FN :  43 from  6214\n",
      "\n",
      "[+] \t Metrics : \n",
      "\t[-]  Accuracy Score (train_test_split):  39.96 %\n",
      "\t[-]  Accuracy Score (k-fold):  39.48 %\n",
      "\t[-]  Classification Error :  60.04 %\n",
      "\t[-]  Recall :  98.20 %\n",
      "\t[-]  Specificity :  38.84 %\n",
      "\t[-]  False Positive Rate :  61.16 %\n",
      "\t[-]  Precision :  3.68 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an instance of kNN classifier\n",
    "# the number of neighbors (k= 350) \n",
    "knn_model = KNeighborsClassifier(n_neighbors=350) \n",
    "\n",
    "# Train the kNN model (x_train-train_features,y_train-train_labels)\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"K-Nearest Neighbors training is complete,\\n predictions: \")\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = knn_model.predict(X_test)\n",
    "\n",
    "print('[+] \\t Classification accuracy : ', \"{:.2f}\".format(metrics.accuracy_score(y_test, predictions) * 100), '%\\n')\n",
    "\n",
    "print('[+] \\t Percentage of Anomaly requests in test set : ', \"{:.2f}\".format(y_test.mean() * 100), '%\\n')\n",
    "print('[+] \\t Percentage of Normal requests in test set : ', \"{:.2f}\".format((1 - y_test.mean()) * 100), '%\\n')\n",
    "\n",
    "confusion = metrics.confusion_matrix(y_test, predictions)\n",
    "TP = confusion[0, 0]\n",
    "TN = confusion[1, 1]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "\n",
    "print('[+] \\t TP : ', TP, 'from ', (TP + TN + FP + FN))\n",
    "print('    \\t TN : ', TN, 'from ', (TP + TN + FP + FN))\n",
    "print('    \\t FP : ', FP, 'from ', (TP + TN + FP + FN))\n",
    "print('    \\t FN : ', FN, 'from ', (TP + TN + FP + FN))\n",
    "\n",
    "print('\\n[+] \\t Metrics : ')\n",
    "print('\\t[-]  Accuracy Score (train_test_split): ', \"{:.2f}\".format(metrics.accuracy_score(y_test, predictions) * 100), '%')\n",
    "print('\\t[-]  Accuracy Score (k-fold): ',\n",
    "      \"{:.2f}\".format(cross_val_score(knn_model, X_train, y_train, cv=100, scoring='accuracy').mean() * 100), '%')\n",
    "\n",
    "print('\\t[-]  Classification Error : ', \"{:.2f}\".format((1 - metrics.accuracy_score(y_test, predictions)) * 100), '%')\n",
    "print('\\t[-]  Recall : ', \"{:.2f}\".format(metrics.recall_score(y_test, predictions) * 100), '%')\n",
    "specificity = TN / (TN + FP)\n",
    "print('\\t[-]  Specificity : ', \"{:.2f}\".format(specificity * 100), '%')\n",
    "false_positive_rate = FP / float(TN + FP)\n",
    "print('\\t[-]  False Positive Rate : ', \"{:.2f}\".format(false_positive_rate * 100), '%')\n",
    "precision = TP / float(TP + FP)\n",
    "print('\\t[-]  Precision : ', \"{:.2f}\".format(precision * 100), '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2f42c",
   "metadata": {},
   "source": [
    "## TensorFlow neural network, with feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4392f",
   "metadata": {},
   "source": [
    "A deep learning model using a Dense Neural Network (DNN), also known as a Fully Connected Network. This type of neural network architecture is characterized by its layers of neurons where each neuron in one layer is connected to all neurons in the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e58bf6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1963/1963 [==============================] - 4s 2ms/step - loss: 0.3077 - accuracy: 0.8881 - recall: 0.8120 - precision: 0.9527 - val_loss: 0.2406 - val_accuracy: 0.9332 - val_recall: 0.9072 - val_precision: 0.9540\n",
      "Epoch 2/10\n",
      "1963/1963 [==============================] - 3s 1ms/step - loss: 0.2297 - accuracy: 0.9305 - recall: 0.9036 - precision: 0.9521 - val_loss: 0.2227 - val_accuracy: 0.9329 - val_recall: 0.9095 - val_precision: 0.9512\n",
      "Epoch 3/10\n",
      "1963/1963 [==============================] - 3s 1ms/step - loss: 0.2237 - accuracy: 0.9312 - recall: 0.9049 - precision: 0.9523 - val_loss: 0.2161 - val_accuracy: 0.9330 - val_recall: 0.9069 - val_precision: 0.9538\n",
      "Epoch 4/10\n",
      "1963/1963 [==============================] - 3s 1ms/step - loss: 0.2231 - accuracy: 0.9309 - recall: 0.9049 - precision: 0.9518 - val_loss: 0.2216 - val_accuracy: 0.9331 - val_recall: 0.9087 - val_precision: 0.9524\n",
      "Epoch 5/10\n",
      "1963/1963 [==============================] - 3s 1ms/step - loss: 0.2208 - accuracy: 0.9312 - recall: 0.9053 - precision: 0.9520 - val_loss: 0.2148 - val_accuracy: 0.9330 - val_recall: 0.9095 - val_precision: 0.9514\n",
      "Epoch 6/10\n",
      "1963/1963 [==============================] - 3s 1ms/step - loss: 0.2196 - accuracy: 0.9310 - recall: 0.9054 - precision: 0.9515 - val_loss: 0.2142 - val_accuracy: 0.9329 - val_recall: 0.9085 - val_precision: 0.9521\n",
      "Epoch 7/10\n",
      "1963/1963 [==============================] - 3s 1ms/step - loss: 0.2187 - accuracy: 0.9311 - recall: 0.9056 - precision: 0.9515 - val_loss: 0.2314 - val_accuracy: 0.9322 - val_recall: 0.9025 - val_precision: 0.9565\n",
      "Epoch 8/10\n",
      "1963/1963 [==============================] - 3s 1ms/step - loss: 0.2163 - accuracy: 0.9312 - recall: 0.9050 - precision: 0.9522 - val_loss: 0.2111 - val_accuracy: 0.9329 - val_recall: 0.9079 - val_precision: 0.9526\n",
      "Epoch 9/10\n",
      "1963/1963 [==============================] - 3s 1ms/step - loss: 0.2163 - accuracy: 0.9309 - recall: 0.9057 - precision: 0.9510 - val_loss: 0.2113 - val_accuracy: 0.9332 - val_recall: 0.9096 - val_precision: 0.9518\n",
      "Epoch 10/10\n",
      "1963/1963 [==============================] - 3s 1ms/step - loss: 0.2159 - accuracy: 0.9312 - recall: 0.9062 - precision: 0.9511 - val_loss: 0.2106 - val_accuracy: 0.9329 - val_recall: 0.9091 - val_precision: 0.9516\n",
      "Test Accuracy: 0.9276\n",
      "Recall: 0.9034\n",
      "Precision: 0.9472\n",
      "614/614 [==============================] - 1s 930us/step\n",
      "Confusion Matrix:\n",
      "[[9475  487]\n",
      " [ 934 8730]]\n"
     ]
    }
   ],
   "source": [
    "# The number of the features\n",
    "n_features = X_train.shape[1]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Recall', 'Precision'])\n",
    "# fit the model\n",
    "\n",
    "# use asarray to convert the dataframe to array\n",
    "X_train_1 = np.asarray(X_train)\n",
    "y_train_1 = np.asarray(y_train)\n",
    "\n",
    "model.fit(X_train_1, y_train_1, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "# evaluate the model\n",
    "X_test_1 = np.asarray(X_test)\n",
    "y_test_1 = np.asarray(y_test)\n",
    "\n",
    "loss, accuracy, recall, precision = model.evaluate(X_test_1, y_test_1, verbose=0)\n",
    "\n",
    "print('Test Accuracy: %.4f' % accuracy)\n",
    "print('Recall: %.4f' % recall)\n",
    "print('Precision: %.4f' % precision)\n",
    "\n",
    "y_pred_classes = (model.predict(X_test_1) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_1, y_pred_classes)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e694de5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Precision: 0.9581961816984859\n",
      "Ensemble Model Recall: 0.9036630794701986\n",
      "Ensemble Model Accuracy: 0.9331499031896464\n",
      "Ensemble Model Accuracy: 0.9331499031896464\n",
      "Ensemble Model Confusion Matrix: [[9581  381]\n",
      " [ 931 8733]]\n",
      "Ensemble Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94      9962\n",
      "           1       0.96      0.90      0.93      9664\n",
      "\n",
      "    accuracy                           0.93     19626\n",
      "   macro avg       0.93      0.93      0.93     19626\n",
      "weighted avg       0.93      0.93      0.93     19626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "\n",
    "# Initialize individual models\n",
    "log_clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "dt_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "svm_clf = GaussianNB() # Enable probability for soft voting\n",
    "#svm_clf = ExtraTreesClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Consider including your previously defined kNN model\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=350)\n",
    "\n",
    "# load my model from .pkl file\n",
    "# with open('knn_model_counter.pkl', 'rb') as f:\n",
    "#      knn_model = pkl.load(f)\n",
    "\n",
    "\n",
    "# Create ensemble model using soft voting\n",
    "ensemble_clf = VotingClassifier(estimators=[\n",
    "    ('lr', log_clf),\n",
    "    ('dt', dt_clf),\n",
    "    ('svc', dt_clf),\n",
    "    # ('knn', knn_model)  # Add your kNN model\n",
    "    ], voting='soft')\n",
    "\n",
    "ensemble_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "ensemble_predictions = ensemble_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "ensemble_CMatrix = confusion_matrix(y_test, ensemble_predictions)\n",
    "\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "ensemble_precision = precision_score(y_test, ensemble_predictions)\n",
    "recall_score = metrics.recall_score(y_test, ensemble_predictions)\n",
    "ensemble_CMatrix = confusion_matrix(y_test, ensemble_predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Ensemble Model Precision:\", ensemble_precision)\n",
    "print(\"Ensemble Model Recall:\", recall_score)\n",
    "print(\"Ensemble Model Accuracy:\", ensemble_accuracy)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Ensemble Model Accuracy:\", ensemble_accuracy)\n",
    "print(\"Ensemble Model Confusion Matrix:\", ensemble_CMatrix)\n",
    "print(\"Ensemble Classification Report:\\n\", classification_report(y_test, ensemble_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
