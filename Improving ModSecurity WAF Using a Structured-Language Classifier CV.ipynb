{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff36039",
   "metadata": {},
   "source": [
    "# Improving ModSecurity WAF Using a Structured-Language Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50d454",
   "metadata": {},
   "source": [
    "## For Computer Science B.Sc. Ariel University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f4593e",
   "metadata": {},
   "source": [
    "By Maor Saadon, Shahar Zaidel, Eden Mor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56246420",
   "metadata": {},
   "source": [
    "# Introdaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936f597",
   "metadata": {},
   "source": [
    "In the ever-evolving landscape of cybersecurity, Web Application Firewalls (WAF) stand as critical defenders against malicious web requests. Traditionally, WAFs rely on predefined rule sets to identify and block potential threats. However, the complexity and sophistication of modern cyber attacks often outpace the capabilities of these rule-based systems, necessitating more dynamic and adaptive approaches. Machine learning (ML), particularly deep learning, has emerged as a promising solution, offering the potential to significantly enhance the detection and mitigation capabilities of WAFs through its ability to learn from data and identify complex patterns.\n",
    "\n",
    "Our research explores the integration of machine learning techniques into the domain of web application security, focusing on the comparative analysis of various ML models in classifying web requests. We delve into models based on advanced text vectorization techniques, such as TF-IDF and Word2Vec, which transform web requests into numerical vectors that capture their semantic essence. These vectors are then utilized by machine learning algorithms, including the traditional k-Nearest Neighbors (kNN) algorithm, to distinguish between benign and malicious requests effectively.\n",
    "\n",
    "Central to our investigation is the exploration of deep learning models in contrast to traditional ML models. Deep learning's capacity for learning hierarchical representations offers a nuanced approach to understanding and classifying web traffic. This leads us to our guiding research question: Does a machine learning model enhanced with deep learning techniques outperform traditional models in the context of Web Application Firewalls?\n",
    "\n",
    "By examining the efficacy of deep learning models against their non-deep learning counterparts, our study aims to shed light on their relative performance and potential for improving the security mechanisms of WAFs. Through this comparative analysis, we aspire to contribute valuable insights into the advancement of cybersecurity methodologies, specifically in enhancing the resilience of web applications against the growing threat of cyber attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f46ca",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d07f0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\maor1\\anaconda3\\lib\\site-packages (2.15.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maor1\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da6b87b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shaha\\PycharmProjects\\CMD_final_project\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import getopt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pickle as pkl\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08339edc",
   "metadata": {},
   "source": [
    "## Feature extraction functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ba8cf",
   "metadata": {},
   "source": [
    "We will use the following feature extraction functions to extract features from the HTTP request:\n",
    "1. calculate_alphanumeric_ratio - \n",
    "2. calculate_input_length - \n",
    "3. calculate_special_character_ratio - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7a5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alphanumeric_ratio(payload):\n",
    "    alphanumeric_characters = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "    alphanumeric_count = sum(1 for char in payload if char in alphanumeric_characters)\n",
    "    payload_length = len(payload)\n",
    "    input_length = max(payload_length, 1)  # Avoid division by zero if payload_length is 0\n",
    "    alphanumeric_ratio = (alphanumeric_count / input_length) * 10\n",
    "    return alphanumeric_ratio\n",
    "\n",
    "def calculate_input_length(payload):\n",
    "    input_length = len(payload)\n",
    "    return input_length\n",
    "\n",
    "def calculate_special_character_ratio(payload):\n",
    "    alphanumeric_characters = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "    special_count = sum(1 for char in payload if char not in alphanumeric_characters)\n",
    "    payload_length = len(payload)\n",
    "    input_length = max(payload_length, 1)  # Avoid division by zero if payload_length is 0\n",
    "    special_ratio = (special_count / input_length) * 100\n",
    "    return special_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3767a62",
   "metadata": {},
   "source": [
    "# Load and Procces the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b802d",
   "metadata": {},
   "source": [
    "The dataset is a CSV file with two columns: payload and label. The payload column contains the http request payload and the label column contains the label of the http request. The label is 1 if the request is malicious and 0 if the request is benign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61629e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Dataset Shape: (35602, 5)\n",
      "Balanced Dataset Distribution: label\n",
      "0    28150\n",
      "1     7452\n",
      "Name: count, dtype: int64\n",
      "Balanced Dataset Head (First 10 Rows):\n",
      "                                             payload  payload_len       alpha  \\\n",
      "0  modo=registro&login=bauer&password=P41DiD3M31t...          264  100.000000   \n",
      "1  modo=insertar&precio=588&B1=Pasar+por+cajaparo...          152   66.355140   \n",
      "2  modo=entrar&login=flaherty&pwd=j%2Baqu%E9s&rem...           65   89.285714   \n",
      "3  id=%3F&nombre=Queso+Manchego&precio=85&cantida...           73   94.444444   \n",
      "4                                                NaN            0    0.000000   \n",
      "5                                                NaN            0    0.000000   \n",
      "6                                  B2=Vaciar+carrito           17  100.000000   \n",
      "7    id=%27%29%3Bwaitfor+delay+%270%3A0%3A15%27%3B--           47   64.285714   \n",
      "8  modo=registro&login=schreife&password=EstOrnin...          257   98.675497   \n",
      "9  id=3&nombre=Queso+Manchego&precio=100&cantidad...           72  100.000000   \n",
      "\n",
      "   non_alpha  label  \n",
      "0   0.000000      0  \n",
      "1  13.084112      1  \n",
      "2  10.714286      1  \n",
      "3   5.555556      1  \n",
      "4   0.000000      0  \n",
      "5   0.000000      0  \n",
      "6   0.000000      0  \n",
      "7  35.714286      1  \n",
      "8   1.324503      0  \n",
      "9   0.000000      0  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('CSIC.csv')\n",
    "df = df.drop(['method','url','attack_feature'], axis=1)\n",
    "\n",
    "# Shuffle the combined dataset to mix malicious and benign URLs\n",
    "balanced_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print('Balanced Dataset Shape:', balanced_df.shape)\n",
    "print('Balanced Dataset Distribution:', balanced_df['label'].value_counts())\n",
    "print('Balanced Dataset Head (First 10 Rows):')\n",
    "print(balanced_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45757f91",
   "metadata": {},
   "source": [
    "## Start testing with the options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69516ab5",
   "metadata": {},
   "source": [
    "We'll split the truncated dataset into a training set and a test set (80% training, 20% test) and use the training set to train the model and the test set to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcadf05",
   "metadata": {},
   "source": [
    "# W2V Vectorizer preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983948a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e7554f",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1174eed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaha\\PycharmProjects\\CMD_final_project\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction...\n",
      "Balanced Dataset Shape: (35602, 5003)\n",
      "Balanced Dataset:\n",
      "          b2=%22%3e%3c%21--%23exec+cmd%3d%22dir+%22--%3e%3c  \\\n",
      "0      0                                                  0   \n",
      "1      0                                                  0   \n",
      "2      0                                                  0   \n",
      "3      0                                                  0   \n",
      "4      1                                                  0   \n",
      "...   ..                                                ...   \n",
      "35597  1                                                  0   \n",
      "35598  1                                                  0   \n",
      "35599  1                                                  0   \n",
      "35600  0                                                  0   \n",
      "35601  1                                                  0   \n",
      "\n",
      "       b2=%22+and+%221%22%3d%221  b2=%2500  b2=%2520  b2=%252b  \\\n",
      "0                              0         0         0         0   \n",
      "1                              0         0         0         0   \n",
      "2                              0         0         0         0   \n",
      "3                              0         0         0         0   \n",
      "4                              0         0         0         0   \n",
      "...                          ...       ...       ...       ...   \n",
      "35597                          0         0         0         0   \n",
      "35598                          0         0         0         0   \n",
      "35599                          0         0         0         0   \n",
      "35600                          0         0         0         0   \n",
      "35601                          0         0         0         0   \n",
      "\n",
      "       b2=%253cscript%253ealert%2528%2522paros%2522%2529%253b%253c%252fscript%253e  \\\n",
      "0                                                      0                             \n",
      "1                                                      0                             \n",
      "2                                                      0                             \n",
      "3                                                      0                             \n",
      "4                                                      0                             \n",
      "...                                                  ...                             \n",
      "35597                                                  0                             \n",
      "35598                                                  0                             \n",
      "35599                                                  0                             \n",
      "35600                                                  0                             \n",
      "35601                                                  0                             \n",
      "\n",
      "       b2=%253f  b2=%257c  b2=%27%29%3bwaitfor+delay+%270%3a0%3a15%27%3b--  \\\n",
      "0             0         0                                                0   \n",
      "1             0         0                                                0   \n",
      "2             0         0                                                0   \n",
      "3             0         0                                                0   \n",
      "4             0         0                                                0   \n",
      "...         ...       ...                                              ...   \n",
      "35597         0         0                                                0   \n",
      "35598         0         0                                                0   \n",
      "35599         0         0                                                0   \n",
      "35600         0         0                                                0   \n",
      "35601         0         0                                                0   \n",
      "\n",
      "       ...  orderparser  samples  server  src  weblogic81  xml  xmlfile=c:  \\\n",
      "0      ...            0        0       0    0           0    0           0   \n",
      "1      ...            0        0       0    0           0    0           0   \n",
      "2      ...            0        0       0    0           0    0           0   \n",
      "3      ...            0        0       0    0           0    0           0   \n",
      "4      ...            0        0       0    0           0    0           0   \n",
      "...    ...          ...      ...     ...  ...         ...  ...         ...   \n",
      "35597  ...            0        0       0    0           0    0           0   \n",
      "35598  ...            0        0       0    0           0    0           0   \n",
      "35599  ...            0        0       0    0           0    0           0   \n",
      "35600  ...            0        0       0    0           0    0           0   \n",
      "35601  ...            0        0       0    0           0    0           0   \n",
      "\n",
      "       payload_len     alpha  non_alpha  \n",
      "0            264.0  8.598485  14.015152  \n",
      "1            152.0  8.618421  13.815789  \n",
      "2             65.0  8.153846  18.461538  \n",
      "3             73.0  8.082192  19.178082  \n",
      "4              0.0  0.000000   0.000000  \n",
      "...            ...       ...        ...  \n",
      "35597          0.0  0.000000   0.000000  \n",
      "35598          0.0  0.000000   0.000000  \n",
      "35599          0.0  0.000000   0.000000  \n",
      "35600         17.0  8.823529  11.764706  \n",
      "35601          0.0  0.000000   0.000000  \n",
      "\n",
      "[35602 rows x 5003 columns]\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN values with an empty string\n",
    "def custom_tokenizer(text):\n",
    "    return text.split('/')\n",
    "\n",
    "# Fill NaN values with an empty string\n",
    "balanced_df['payload'] = balanced_df['payload'].fillna(\"\")\n",
    "\n",
    "# Initialize the CountVectorizer with custom tokenizer\n",
    "vectorizer = CountVectorizer(max_features=5000, max_df=0.85, tokenizer=custom_tokenizer, stop_words=['https', 'localhost:8080']) # , max_df=0.85, tokenizer=custom_tokenizer, stop_words=['https', 'localhost:8080']\n",
    "\n",
    "# Fit and transform the 'payload' column\n",
    "X = vectorizer.fit_transform(balanced_df['payload'])\n",
    "\n",
    "# Convert the result to an array if needed\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Optionally, convert to DataFrame for better readability\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "X_df = pd.DataFrame(X_array, columns=feature_names)\n",
    "\n",
    "# Apply feature extraction\n",
    "print('Feature extraction...')\n",
    "features = balanced_df['payload'].apply(lambda x: pd.Series({\n",
    "    'payload_len': calculate_input_length(str(x)),\n",
    "    'alpha': calculate_alphanumeric_ratio(str(x)),\n",
    "    'non_alpha': calculate_special_character_ratio(str(x)),\n",
    "    }))\n",
    "\n",
    "\n",
    "# Concatenate original DF with features\n",
    "X_df = pd.concat([X_df, features], axis=1)\n",
    "print('Balanced Dataset Shape:', X_df.shape)\n",
    "print('Balanced Dataset:')\n",
    "print(X_df)\n",
    "\n",
    "# Define y\n",
    "y = balanced_df['label'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840aabe4",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b02d59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors training is complete,\n",
      " predictions: \n",
      "[+] \t Classification accuracy :  93.26 %\n",
      "\n",
      "[+] \t Percentage of Anomaly requests in test set :  20.95 %\n",
      "\n",
      "[+] \t Percentage of Normal requests in test set :  79.05 %\n",
      "\n",
      "[+] \t TP :  1045 from  7121\n",
      "    \t TN :  5596 from  7121\n",
      "    \t FP :  33 from  7121\n",
      "    \t FN :  447 from  7121\n",
      "\n",
      "[+] \t Metrics : \n",
      "\t[-]  Accuracy Score (train_test_split):  93.26 %\n",
      "\t[-]  Accuracy Score (k-fold):  93.08 %\n",
      "\t[-]  Classification Error :  6.74 %\n",
      "\t[-]  Recall :  70.04 %\n",
      "\t[-]  Specificity :  99.41 %\n",
      "\t[-]  False Positive Rate :  0.59 %\n",
      "\t[-]  Precision :  96.94 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an instance of kNN classifier\n",
    "# the number of neighbors (k= 350) \n",
    "knn_model = KNeighborsClassifier(n_neighbors=350) \n",
    "\n",
    "# Train the kNN model (x_train-train_features,y_train-train_labels)\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"K-Nearest Neighbors training is complete,\\n predictions: \")\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = knn_model.predict(X_test)\n",
    "\n",
    "# #Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, predictions)\n",
    "# CMatrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# # Print all evaluation metrics\n",
    "# print(\"Accuracy:\", accuracy,\"Confusion Matrix:\", CMatrix ,\"\\n\",\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# # Extract recall, precision, and F1-score from the classification report\n",
    "# classification_dict = classification_report(y_test, predictions, output_dict=True)\n",
    "# recall = classification_dict['1']['recall']  # Recall for the positive class (malicious)\n",
    "# precision = classification_dict['1']['precision']  # Precision for the positive class (malicious)\n",
    "# f1_score = classification_dict['1']['f1-score']  # F1-score for the positive class (malicious)\n",
    "\n",
    "# # Dump the model to disk\n",
    "# with open('knn_model_counter.pkl', 'wb') as f:\n",
    "#     pkl.dump(knn_model, f)\n",
    "\n",
    "# print('K-Nearest Neighbors model saved to disk.')\n",
    "\n",
    "# print(\"Recall:\", recall)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"F1 score:\", f1_score)\n",
    "\n",
    "\n",
    "#y_test = y_test.values.ravel()\n",
    "\n",
    "print('[+] \\t Classification accuracy : ', \"{:.2f}\".format(metrics.accuracy_score(y_test, predictions) * 100), '%\\n')\n",
    "\n",
    "print('[+] \\t Percentage of Anomaly requests in test set : ', \"{:.2f}\".format(y_test.mean() * 100), '%\\n')\n",
    "print('[+] \\t Percentage of Normal requests in test set : ', \"{:.2f}\".format((1 - y_test.mean()) * 100), '%\\n')\n",
    "\n",
    "confusion = metrics.confusion_matrix(y_test, predictions)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "\n",
    "print('[+] \\t TP : ', TP, 'from ', (TP + TN + FP + FN))\n",
    "print('    \\t TN : ', TN, 'from ', (TP + TN + FP + FN))\n",
    "print('    \\t FP : ', FP, 'from ', (TP + TN + FP + FN))\n",
    "print('    \\t FN : ', FN, 'from ', (TP + TN + FP + FN))\n",
    "\n",
    "print('\\n[+] \\t Metrics : ')\n",
    "print('\\t[-]  Accuracy Score (train_test_split): ', \"{:.2f}\".format(metrics.accuracy_score(y_test, predictions) * 100), '%')\n",
    "print('\\t[-]  Accuracy Score (k-fold): ',\n",
    "      \"{:.2f}\".format(cross_val_score(knn_model, X_train, y_train, cv=100, scoring='accuracy').mean() * 100), '%')\n",
    "\n",
    "print('\\t[-]  Classification Error : ', \"{:.2f}\".format((1 - metrics.accuracy_score(y_test, predictions)) * 100), '%')\n",
    "print('\\t[-]  Recall : ', \"{:.2f}\".format(metrics.recall_score(y_test, predictions) * 100), '%')\n",
    "specificity = TN / (TN + FP)\n",
    "print('\\t[-]  Specificity : ', \"{:.2f}\".format(specificity * 100), '%')\n",
    "false_positive_rate = FP / float(TN + FP)\n",
    "print('\\t[-]  False Positive Rate : ', \"{:.2f}\".format(false_positive_rate * 100), '%')\n",
    "precision = TP / float(TP + FP)\n",
    "print('\\t[-]  Precision : ', \"{:.2f}\".format(precision * 100), '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2f42c",
   "metadata": {},
   "source": [
    "## TensorFlow neural network, with feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4392f",
   "metadata": {},
   "source": [
    "A deep learning model using a Dense Neural Network (DNN), also known as a Fully Connected Network. This type of neural network architecture is characterized by its layers of neurons where each neuron in one layer is connected to all neurons in the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e58bf6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "712/712 [==============================] - 2s 2ms/step - loss: 0.2860 - accuracy: 0.9290 - recall: 0.8068 - precision: 0.8485 - val_loss: 0.0935 - val_accuracy: 0.9856 - val_recall: 0.9321 - val_precision: 0.9972\n",
      "Epoch 2/10\n",
      "712/712 [==============================] - 1s 2ms/step - loss: 0.0738 - accuracy: 0.9855 - recall: 0.9435 - precision: 0.9869 - val_loss: 0.0653 - val_accuracy: 0.9914 - val_recall: 0.9613 - val_precision: 0.9964\n",
      "Epoch 3/10\n",
      "712/712 [==============================] - 1s 2ms/step - loss: 0.0493 - accuracy: 0.9934 - recall: 0.9737 - precision: 0.9947 - val_loss: 0.0434 - val_accuracy: 0.9919 - val_recall: 0.9604 - val_precision: 1.0000\n",
      "Epoch 4/10\n",
      "712/712 [==============================] - 1s 2ms/step - loss: 0.0399 - accuracy: 0.9947 - recall: 0.9777 - precision: 0.9970 - val_loss: 0.0507 - val_accuracy: 0.9714 - val_recall: 0.9854 - val_precision: 0.8870\n",
      "Epoch 5/10\n",
      "712/712 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9964 - recall: 0.9854 - precision: 0.9975 - val_loss: 0.0306 - val_accuracy: 0.9935 - val_recall: 0.9690 - val_precision: 0.9991\n",
      "Epoch 6/10\n",
      "712/712 [==============================] - 1s 2ms/step - loss: 0.0262 - accuracy: 0.9973 - recall: 0.9879 - precision: 0.9992 - val_loss: 0.0293 - val_accuracy: 0.9940 - val_recall: 0.9837 - val_precision: 0.9871\n",
      "Epoch 7/10\n",
      "712/712 [==============================] - 1s 2ms/step - loss: 0.0222 - accuracy: 0.9975 - recall: 0.9898 - precision: 0.9983 - val_loss: 0.0227 - val_accuracy: 0.9951 - val_recall: 0.9776 - val_precision: 0.9982\n",
      "Epoch 8/10\n",
      "712/712 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9975 - recall: 0.9898 - precision: 0.9985 - val_loss: 0.0271 - val_accuracy: 0.9926 - val_recall: 0.9682 - val_precision: 0.9956\n",
      "Epoch 9/10\n",
      "712/712 [==============================] - 1s 2ms/step - loss: 0.0153 - accuracy: 0.9978 - recall: 0.9906 - precision: 0.9989 - val_loss: 0.0260 - val_accuracy: 0.9949 - val_recall: 0.9845 - val_precision: 0.9905\n",
      "Epoch 10/10\n",
      "712/712 [==============================] - 1s 2ms/step - loss: 0.0138 - accuracy: 0.9978 - recall: 0.9915 - precision: 0.9983 - val_loss: 0.0185 - val_accuracy: 0.9940 - val_recall: 0.9716 - val_precision: 0.9991\n",
      "Test Accuracy: 0.993\n",
      "Recall: 0.967\n",
      "Precision: 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Define the model\n",
    "# model = Sequential([\n",
    "#     Dense(512, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer with input_shape matching your features\n",
    "#     Dropout(0.5),  # Dropout for regularization\n",
    "#     Dense(256, activation='relu'),  # Hidden layer\n",
    "#     Dropout(0.5),  # Dropout for regularization\n",
    "#     Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "# ])\n",
    "# model.\n",
    "#\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',  # Use 'categorical_crossentropy' for multi-class classification\n",
    "#               metrics=['accuracy'])\n",
    "#\n",
    "# # Train the model\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                     epochs=10,\n",
    "#                     batch_size=32,\n",
    "#                     validation_split=0.2,  # Use a part of the training set for validation\n",
    "#                     verbose=1)\n",
    "#\n",
    "# # Evaluate the model on the test set\n",
    "# test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "# print('\\nTest accuracy:', test_acc)\n",
    "# print('\\nTest loss:', test_loss)\n",
    "\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Recall', 'Precision'])\n",
    "# fit the model\n",
    "\n",
    "# use asarray to convert the dataframe to array\n",
    "X_train_1 = np.asarray(X_train)\n",
    "y_train_1 = np.asarray(y_train)\n",
    "\n",
    "model.fit(X_train_1, y_train_1, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "# evaluate the model\n",
    "X_test_1 = np.asarray(X_test)\n",
    "y_test_1 = np.asarray(y_test)\n",
    "\n",
    "loss, accuracy, recall, precision = model.evaluate(X_test_1, y_test_1, verbose=0)\n",
    "# print the accuracy in a different way\n",
    "print('Test Accuracy: %.3f' % accuracy)\n",
    "print('Recall: %.3f' % recall)\n",
    "print('Precision: %.3f' % precision)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e694de5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy: 0.8313439123718579\n",
      "Ensemble Model Confusion Matrix: [[5629    0]\n",
      " [1201  291]]\n",
      "Ensemble Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90      5629\n",
      "           1       1.00      0.20      0.33      1492\n",
      "\n",
      "    accuracy                           0.83      7121\n",
      "   macro avg       0.91      0.60      0.62      7121\n",
      "weighted avg       0.86      0.83      0.78      7121\n",
      "\n",
      "Ensemble model saved to disk.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "\n",
    "# Initialize individual models\n",
    "log_clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "dt_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "#svm_clf = GaussianNB() # Enable probability for soft voting\n",
    "svm_clf = ExtraTreesClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Create ensemble model using soft voting\n",
    "ensemble_clf = VotingClassifier(estimators=[\n",
    "    ('lr', log_clf),\n",
    "    ('dt', dt_clf),\n",
    "    ('svc', dt_clf),\n",
    "    ], voting='soft')\n",
    "\n",
    "ensemble_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "ensemble_predictions = ensemble_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "ensemble_CMatrix = confusion_matrix(y_test, ensemble_predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Ensemble Model Accuracy:\", ensemble_accuracy)\n",
    "print(\"Ensemble Model Confusion Matrix:\", ensemble_CMatrix)\n",
    "print(\"Ensemble Classification Report:\\n\", classification_report(y_test, ensemble_predictions))\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the ensemble model to disk\n",
    "with open('ensemble_model.pkl', 'wb') as f:\n",
    "    pickle.dump(ensemble_clf, f)\n",
    "\n",
    "print('Ensemble model saved to disk.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
