{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff36039",
   "metadata": {},
   "source": [
    "# Improving ModSecurity WAF Using a Structured-Language Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50d454",
   "metadata": {},
   "source": [
    "## For Computer Science B.Sc. Ariel University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f4593e",
   "metadata": {},
   "source": [
    "By Maor Saadon, Shahar Zaidel, Eden Mor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56246420",
   "metadata": {},
   "source": [
    "# Introdaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936f597",
   "metadata": {},
   "source": [
    "In the ever-evolving landscape of cybersecurity, Web Application Firewalls (WAF) stand as critical defenders against malicious web requests. Traditionally, WAFs rely on predefined rule sets to identify and block potential threats. However, the complexity and sophistication of modern cyber attacks often outpace the capabilities of these rule-based systems, necessitating more dynamic and adaptive approaches. Machine learning (ML), particularly deep learning, has emerged as a promising solution, offering the potential to significantly enhance the detection and mitigation capabilities of WAFs through its ability to learn from data and identify complex patterns.\n",
    "\n",
    "Our research explores the integration of machine learning techniques into the domain of web application security, focusing on the comparative analysis of various ML models in classifying web requests. We delve into models based on advanced text vectorization techniques, such as TF-IDF and Word2Vec, which transform web requests into numerical vectors that capture their semantic essence. These vectors are then utilized by machine learning algorithms, including the traditional k-Nearest Neighbors (kNN) algorithm, to distinguish between benign and malicious requests effectively.\n",
    "\n",
    "Central to our investigation is the exploration of deep learning models in contrast to traditional ML models. Deep learning's capacity for learning hierarchical representations offers a nuanced approach to understanding and classifying web traffic. This leads us to our guiding research question: Does a machine learning model enhanced with deep learning techniques outperform traditional models in the context of Web Application Firewalls?\n",
    "\n",
    "By examining the efficacy of deep learning models against their non-deep learning counterparts, our study aims to shed light on their relative performance and potential for improving the security mechanisms of WAFs. Through this comparative analysis, we aspire to contribute valuable insights into the advancement of cybersecurity methodologies, specifically in enhancing the resilience of web applications against the growing threat of cyber attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f46ca",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d07f0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\maor1\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maor1\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da6b87b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import getopt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pickle as pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08339edc",
   "metadata": {},
   "source": [
    "## Feature extraction functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ba8cf",
   "metadata": {},
   "source": [
    "We will use the following feature extraction functions to extract features from the HTTP request:\n",
    "1. calculate_alphanumeric_ratio - \n",
    "2. calculate_input_length - \n",
    "3. calculate_special_character_ratio - \n",
    "4. calculate_url_weight - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f7a5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alphanumeric_ratio(payload):\n",
    "    alphanumeric_characters = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "    alphanumeric_count = sum(1 for char in payload if char in alphanumeric_characters)\n",
    "    payload_length = len(payload)\n",
    "    input_length = max(payload_length, 1)  # Avoid division by zero if payload_length is 0\n",
    "    alphanumeric_ratio = (alphanumeric_count / input_length) * 10\n",
    "    return alphanumeric_ratio\n",
    "\n",
    "def calculate_input_length(payload):\n",
    "    input_length = len(payload)\n",
    "    return input_length\n",
    "\n",
    "def calculate_special_character_ratio(payload):\n",
    "    alphanumeric_characters = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "    special_count = sum(1 for char in payload if char not in alphanumeric_characters)\n",
    "    payload_length = len(payload)\n",
    "    input_length = max(payload_length, 1)  # Avoid division by zero if payload_length is 0\n",
    "    special_ratio = (special_count / input_length) * 100\n",
    "    return special_ratio\n",
    "\n",
    "\n",
    "def calculate_url_weight(url, discovered_malicious):\n",
    "    # Define weights for discovered malicious\n",
    "    malicious_weights = {\n",
    "        \"special_character\": 10,\n",
    "        \"attack_word\": 50,\n",
    "        \"unauthorized_resource_access\": 200\n",
    "    }\n",
    "    # Initialize URL weight\n",
    "    url_weight = 0\n",
    "    # Check if any discovered malicious is present in the URL\n",
    "    for malicious in discovered_malicious:\n",
    "        if malicious in url:\n",
    "            url_weight += malicious_weights.get(malicious, 0)\n",
    "\n",
    "    return url_weight\n",
    "\n",
    "def calculate_attack_weight(row):\n",
    "    # Example of weights, these should be determined based on your analysis\n",
    "    url_weight = row['url_weight']\n",
    "    payload_weight = row['payload_weight']\n",
    "    manipulation = row['manipulation']  # Number of attack words\n",
    "    alphanumeric_ratio = row['alpha'] / (row['alpha'] + row['non_alpha']) if row['alpha'] + row['non_alpha'] > 0 else 0\n",
    "    files_weight = row['files_weight']  # This should be calculated based on your context\n",
    "    attack_weight = url_weight + payload_weight + manipulation + alphanumeric_ratio + files_weight\n",
    "    return attack_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3767a62",
   "metadata": {},
   "source": [
    "# Load and Procces the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b802d",
   "metadata": {},
   "source": [
    "The dataset is a CSV file with two columns: payload and label. The payload column contains the http request payload and the label column contains the label of the http request. The label is 1 if the request is malicious and 0 if the request is benign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61629e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Dataset Shape: (98126, 2)\n",
      "Balanced Dataset Distribution: label\n",
      "0    50000\n",
      "1    48126\n",
      "Name: count, dtype: int64\n",
      "Balanced Dataset Head (First 10 Rows):\n",
      "                                             payload  label\n",
      "0  <iframe  src=\"data:text/html,%3C%73%63%72%69%7...      1\n",
      "1                          /javascript/stackdump.exe      1\n",
      "2  &lt;?import namespace=\\\"t\\\" implementation=\\\"#...      1\n",
      "3  /awstats/awstats.pl?migrate=|echo;wget -p /tmp...      1\n",
      "4                                           /146054/      0\n",
      "5                             123+len(1234)-len(123)      1\n",
      "6                                         /revhosts/      0\n",
      "7                                /javascript/devs.7z      0\n",
      "8                       /javascript/certificate.core      0\n",
      "9                               /javascript/item.swf      0\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('QueriesDataset.csv')\n",
    "\n",
    "# Shuffle the combined dataset to mix malicious and benign URLs\n",
    "balanced_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print('Balanced Dataset Shape:', balanced_df.shape)\n",
    "print('Balanced Dataset Distribution:', balanced_df['label'].value_counts())\n",
    "print('Balanced Dataset Head (First 10 Rows):')\n",
    "print(balanced_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45757f91",
   "metadata": {},
   "source": [
    "## Start testing with the options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69516ab5",
   "metadata": {},
   "source": [
    "We'll split the truncated dataset into a training set and a test set (80% training, 20% test) and use the training set to train the model and the test set to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b97a4",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "573e7006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction...\n",
      "Balanced Dataset Shape: (98126, 30)\n",
      "Balanced Dataset:\n",
      "                                                 payload  label  \\\n",
      "0      <iframe  src=\"data:text/html,%3C%73%63%72%69%7...      1   \n",
      "1                              /javascript/stackdump.exe      1   \n",
      "2      &lt;?import namespace=\\\"t\\\" implementation=\\\"#...      1   \n",
      "3      /awstats/awstats.pl?migrate=|echo;wget -p /tmp...      1   \n",
      "4                                               /146054/      0   \n",
      "...                                                  ...    ...   \n",
      "98121                                           /110992/      0   \n",
      "98122  /recordings/index.php?action=login&languages[n...      1   \n",
      "98123  /scripts/search.jsp?q=%\"<script>alert(13319041...      1   \n",
      "98124                                   /quake_4-teaser/      0   \n",
      "98125                                          /3483441/      0   \n",
      "\n",
      "       alphanumeric_ratio  input_length  special_character_ratio  url_weight  \\\n",
      "0                6.695652         115.0                33.043478         0.0   \n",
      "1                8.800000          25.0                12.000000         0.0   \n",
      "2                7.076923          65.0                29.230769         0.0   \n",
      "3                8.098592         142.0                19.014085         0.0   \n",
      "4                7.500000           8.0                25.000000         0.0   \n",
      "...                   ...           ...                      ...         ...   \n",
      "98121            7.500000           8.0                25.000000         0.0   \n",
      "98122            8.067227         119.0                19.327731         0.0   \n",
      "98123            7.586207          58.0                24.137931         0.0   \n",
      "98124            7.500000          16.0                25.000000         0.0   \n",
      "98125            7.777778           9.0                22.222222         0.0   \n",
      "\n",
      "       alphanumeric_ratio  input_length  special_character_ratio  url_weight  \\\n",
      "0                6.695652         115.0                33.043478         0.0   \n",
      "1                8.800000          25.0                12.000000         0.0   \n",
      "2                7.076923          65.0                29.230769         0.0   \n",
      "3                8.098592         142.0                19.014085         0.0   \n",
      "4                7.500000           8.0                25.000000         0.0   \n",
      "...                   ...           ...                      ...         ...   \n",
      "98121            7.500000           8.0                25.000000         0.0   \n",
      "98122            8.067227         119.0                19.327731         0.0   \n",
      "98123            7.586207          58.0                24.137931         0.0   \n",
      "98124            7.500000          16.0                25.000000         0.0   \n",
      "98125            7.777778           9.0                22.222222         0.0   \n",
      "\n",
      "       ...  special_character_ratio  url_weight  alphanumeric_ratio  \\\n",
      "0      ...                33.043478         0.0            6.695652   \n",
      "1      ...                12.000000         0.0            8.800000   \n",
      "2      ...                29.230769         0.0            7.076923   \n",
      "3      ...                19.014085         0.0            8.098592   \n",
      "4      ...                25.000000         0.0            7.500000   \n",
      "...    ...                      ...         ...                 ...   \n",
      "98121  ...                25.000000         0.0            7.500000   \n",
      "98122  ...                19.327731         0.0            8.067227   \n",
      "98123  ...                24.137931         0.0            7.586207   \n",
      "98124  ...                25.000000         0.0            7.500000   \n",
      "98125  ...                22.222222         0.0            7.777778   \n",
      "\n",
      "       input_length  special_character_ratio  url_weight  alphanumeric_ratio  \\\n",
      "0             115.0                33.043478         0.0            6.695652   \n",
      "1              25.0                12.000000         0.0            8.800000   \n",
      "2              65.0                29.230769         0.0            7.076923   \n",
      "3             142.0                19.014085         0.0            8.098592   \n",
      "4               8.0                25.000000         0.0            7.500000   \n",
      "...             ...                      ...         ...                 ...   \n",
      "98121           8.0                25.000000         0.0            7.500000   \n",
      "98122         119.0                19.327731         0.0            8.067227   \n",
      "98123          58.0                24.137931         0.0            7.586207   \n",
      "98124          16.0                25.000000         0.0            7.500000   \n",
      "98125           9.0                22.222222         0.0            7.777778   \n",
      "\n",
      "       input_length  special_character_ratio  url_weight  \n",
      "0             115.0                33.043478         0.0  \n",
      "1              25.0                12.000000         0.0  \n",
      "2              65.0                29.230769         0.0  \n",
      "3             142.0                19.014085         0.0  \n",
      "4               8.0                25.000000         0.0  \n",
      "...             ...                      ...         ...  \n",
      "98121           8.0                25.000000         0.0  \n",
      "98122         119.0                19.327731         0.0  \n",
      "98123          58.0                24.137931         0.0  \n",
      "98124          16.0                25.000000         0.0  \n",
      "98125           9.0                22.222222         0.0  \n",
      "\n",
      "[98126 rows x 30 columns]\n",
      "Fitting and transforming the TF-IDF Vectorizer to the payloade in the dataset...\n",
      "TF-IDF Vectorization complete.\n",
      "Saving the TF-IDF Vectorizer to disk...\n",
      "TF-IDF Vectorizer saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# Apply feature extraction\n",
    "print('Feature extraction...')\n",
    "features = balanced_df['payload'].apply(lambda x: pd.Series({\n",
    "   'alphanumeric_ratio': calculate_alphanumeric_ratio(str(x)),\n",
    "    'input_length': calculate_input_length(str(x)),\n",
    "    'special_character_ratio': calculate_special_character_ratio(str(x)),\n",
    "    'url_weight': calculate_url_weight(str(x), ['select', 'union', 'insert', 'delete', 'update']),\n",
    "    }))\n",
    "\n",
    "\n",
    "# Concatenate original DF with features\n",
    "balanced_df = pd.concat([balanced_df, features], axis=1)\n",
    "print('Balanced Dataset Shape:', balanced_df.shape)\n",
    "print('Balanced Dataset:')\n",
    "print(balanced_df)\n",
    "\n",
    "# Extracting TF-IDF features from URLs\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to top 5000 features\n",
    "\n",
    "# Before fitting the TF-IDF Vectorizer, replace NaN values with empty strings\n",
    "balanced_df['payload'] = balanced_df['payload'].fillna('')\n",
    "\n",
    "# Fit and transform the TF-IDF vectorizer\n",
    "print('Fitting and transforming the TF-IDF Vectorizer to the payloade in the dataset...')\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(balanced_df['payload'])\n",
    "print('TF-IDF Vectorization complete.')\n",
    "\n",
    "# Convert TF-IDF features from a sparse matrix to a dense format and then to an np.ndarray\n",
    "tfidf_dense = np.asarray(tfidf_features.todense())\n",
    "\n",
    "# Define X for numerical features\n",
    "X_numerical = balanced_df.drop(['label', 'payload'], axis=1).values  # Make sure this matches your feature extraction output\n",
    "\n",
    "# Combining TF-IDF features with numerical features\n",
    "X_combined = np.hstack((X_numerical, tfidf_dense))\n",
    "\n",
    "# Define y\n",
    "y = balanced_df['label'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Saving the TF-IDF Vectorizer to disk...')\n",
    "\n",
    "# Save the TF-IDF vectorizer to disk\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pkl.dump(tfidf_vectorizer, f)\n",
    "\n",
    "print('TF-IDF Vectorizer saved to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840aabe4",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02d59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors training is complete,\n",
      " predictions: \n"
     ]
    }
   ],
   "source": [
    "# Create an instance of kNN classifier\n",
    "# the number of neighbors (k= 350) \n",
    "knn_model = KNeighborsClassifier(n_neighbors=350) \n",
    "\n",
    "# Train the kNN model (x_train-train_features,y_train-train_labels)\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"K-Nearest Neighbors training is complete,\\n predictions: \")\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = knn_model.predict(X_test)\n",
    "\n",
    "#Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "CMatrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Print all evaluation metrics\n",
    "print(\"Accuracy:\", accuracy,\"Confusion Matrix:\", CMatrix ,\"\\n\",\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# Extract recall, precision, and F1-score from the classification report\n",
    "classification_dict = classification_report(y_test, predictions, output_dict=True)\n",
    "recall = classification_dict['1']['recall']  # Recall for the positive class (malicious)\n",
    "precision = classification_dict['1']['precision']  # Precision for the positive class (malicious)\n",
    "f1_score = classification_dict['1']['f1-score']  # F1-score for the positive class (malicious)\n",
    "\n",
    "# Dump the model to disk\n",
    "with open('knn_model.pkl', 'wb') as f:\n",
    "    pkl.dump(knn_model, f)\n",
    "\n",
    "print('K-Nearest Neighbors model saved to disk.')\n",
    "\n",
    "# print(\"Recall:\", recall)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"F1-score:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2f42c",
   "metadata": {},
   "source": [
    "## TensorFlow neural network, with feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4392f",
   "metadata": {},
   "source": [
    "Now let's use a neural network to classify the HTTP requests. We will use the feature extraction functions to extract features from the payloads and then use a neural network to classify the request's payloads. We will use the same training and test sets as before. As the same, the training set is 80% and the test set is 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer with input_shape matching your features\n",
    "    Dropout(0.5),  # Dropout for regularization\n",
    "    Dense(256, activation='relu'),  # Hidden layer\n",
    "    Dropout(0.5),  # Dropout for regularization\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Use 'categorical_crossentropy' for multi-class classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,  # Use a part of the training set for validation\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e694de5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
