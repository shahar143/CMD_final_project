{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff36039",
   "metadata": {},
   "source": [
    "# Improving ModSecurity WAF Using a Structured-Language Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50d454",
   "metadata": {},
   "source": [
    "## For Computer Science B.Sc. Ariel University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f4593e",
   "metadata": {},
   "source": [
    "By Maor Saadon, Shahar Zaidel, Eden Mor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56246420",
   "metadata": {},
   "source": [
    "# Introdaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936f597",
   "metadata": {},
   "source": [
    "In the ever-evolving landscape of cybersecurity, Web Application Firewalls (WAF) stand as critical defenders against malicious web requests. Traditionally, WAFs rely on predefined rule sets to identify and block potential threats. However, the complexity and sophistication of modern cyber attacks often outpace the capabilities of these rule-based systems, necessitating more dynamic and adaptive approaches. Machine learning (ML), particularly deep learning, has emerged as a promising solution, offering the potential to significantly enhance the detection and mitigation capabilities of WAFs through its ability to learn from data and identify complex patterns.\n",
    "\n",
    "Our research explores the integration of machine learning techniques into the domain of web application security, focusing on the comparative analysis of various ML models in classifying web requests. We delve into models based on advanced text vectorization techniques, such as TF-IDF and Word2Vec, which transform web requests into numerical vectors that capture their semantic essence. These vectors are then utilized by machine learning algorithms, including the traditional k-Nearest Neighbors (kNN) algorithm, to distinguish between benign and malicious requests effectively.\n",
    "\n",
    "Central to our investigation is the exploration of deep learning models in contrast to traditional ML models. Deep learning's capacity for learning hierarchical representations offers a nuanced approach to understanding and classifying web traffic. This leads us to our guiding research question: Does a machine learning model enhanced with deep learning techniques outperform traditional models in the context of Web Application Firewalls?\n",
    "\n",
    "By examining the efficacy of deep learning models against their non-deep learning counterparts, our study aims to shed light on their relative performance and potential for improving the security mechanisms of WAFs. Through this comparative analysis, we aspire to contribute valuable insights into the advancement of cybersecurity methodologies, specifically in enhancing the resilience of web applications against the growing threat of cyber attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f46ca",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d07f0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\maor1\\anaconda3\\lib\\site-packages (2.15.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maor1\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\maor1\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da6b87b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shaha\\PycharmProjects\\CMD_final_project\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import getopt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pickle as pkl\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08339edc",
   "metadata": {},
   "source": [
    "## Feature extraction functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ba8cf",
   "metadata": {},
   "source": [
    "We will use the following feature extraction functions to extract features from the HTTP request:\n",
    "1. calculate_alphanumeric_ratio - \n",
    "2. calculate_input_length - \n",
    "3. calculate_special_character_ratio - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7a5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alphanumeric_ratio(payload):\n",
    "    alphanumeric_characters = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "    alphanumeric_count = sum(1 for char in payload if char in alphanumeric_characters)\n",
    "    payload_length = len(payload)\n",
    "    input_length = max(payload_length, 1)  # Avoid division by zero if payload_length is 0\n",
    "    alphanumeric_ratio = (alphanumeric_count / input_length) * 10\n",
    "    return alphanumeric_ratio\n",
    "\n",
    "def calculate_input_length(payload):\n",
    "    input_length = len(payload)\n",
    "    return input_length\n",
    "\n",
    "def calculate_special_character_ratio(payload):\n",
    "    alphanumeric_characters = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "    special_count = sum(1 for char in payload if char not in alphanumeric_characters)\n",
    "    payload_length = len(payload)\n",
    "    input_length = max(payload_length, 1)  # Avoid division by zero if payload_length is 0\n",
    "    special_ratio = (special_count / input_length) * 100\n",
    "    return special_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3767a62",
   "metadata": {},
   "source": [
    "# Load and Procces the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b802d",
   "metadata": {},
   "source": [
    "The dataset is a CSV file with two columns: payload and label. The payload column contains the http request payload and the label column contains the label of the http request. The label is 1 if the request is malicious and 0 if the request is benign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61629e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Dataset Shape: (98126, 2)\n",
      "Balanced Dataset Distribution: label\n",
      "0    50000\n",
      "1    48126\n",
      "Name: count, dtype: int64\n",
      "Balanced Dataset Head (First 10 Rows):\n",
      "                                             payload  label\n",
      "0  <iframe  src=\"data:text/html,%3C%73%63%72%69%7...      1\n",
      "1                          /javascript/stackdump.exe      1\n",
      "2  &lt;?import namespace=\\\"t\\\" implementation=\\\"#...      1\n",
      "3  /awstats/awstats.pl?migrate=|echo;wget -p /tmp...      1\n",
      "4                                           /146054/      0\n",
      "5                             123+len(1234)-len(123)      1\n",
      "6                                         /revhosts/      0\n",
      "7                                /javascript/devs.7z      0\n",
      "8                       /javascript/certificate.core      0\n",
      "9                               /javascript/item.swf      0\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('QueriesDataset.csv')\n",
    "# df = df.drop(['method','url','attack_feature'], axis=1)\n",
    "\n",
    "# Shuffle the combined dataset to mix malicious and benign URLs\n",
    "balanced_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print('Balanced Dataset Shape:', balanced_df.shape)\n",
    "print('Balanced Dataset Distribution:', balanced_df['label'].value_counts())\n",
    "print('Balanced Dataset Head (First 10 Rows):')\n",
    "print(balanced_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45757f91",
   "metadata": {},
   "source": [
    "## Start testing with the options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69516ab5",
   "metadata": {},
   "source": [
    "We'll split the truncated dataset into a training set and a test set (80% training, 20% test) and use the training set to train the model and the test set to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b97a4",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573e7006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction...\n",
      "Balanced Dataset Shape: (98126, 5)\n",
      "Balanced Dataset:\n",
      "                                                 payload  label  payload_len  \\\n",
      "0      <iframe  src=\"data:text/html,%3C%73%63%72%69%7...      1        115.0   \n",
      "1                              /javascript/stackdump.exe      1         25.0   \n",
      "2      &lt;?import namespace=\\\"t\\\" implementation=\\\"#...      1         65.0   \n",
      "3      /awstats/awstats.pl?migrate=|echo;wget -p /tmp...      1        142.0   \n",
      "4                                               /146054/      0          8.0   \n",
      "...                                                  ...    ...          ...   \n",
      "98121                                           /110992/      0          8.0   \n",
      "98122  /recordings/index.php?action=login&languages[n...      1        119.0   \n",
      "98123  /scripts/search.jsp?q=%\"<script>alert(13319041...      1         58.0   \n",
      "98124                                   /quake_4-teaser/      0         16.0   \n",
      "98125                                          /3483441/      0          9.0   \n",
      "\n",
      "          alpha  non_alpha  \n",
      "0      6.695652  33.043478  \n",
      "1      8.800000  12.000000  \n",
      "2      7.076923  29.230769  \n",
      "3      8.098592  19.014085  \n",
      "4      7.500000  25.000000  \n",
      "...         ...        ...  \n",
      "98121  7.500000  25.000000  \n",
      "98122  8.067227  19.327731  \n",
      "98123  7.586207  24.137931  \n",
      "98124  7.500000  25.000000  \n",
      "98125  7.777778  22.222222  \n",
      "\n",
      "[98126 rows x 5 columns]\n",
      "Fitting and transforming the TF-IDF Vectorizer to the payloade in the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaha\\PycharmProjects\\CMD_final_project\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorization complete.\n",
      "Saving the TF-IDF Vectorizer to disk...\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x000001A0C971E840>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPicklingError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 49\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;66;03m# Save the TF-IDF vectorizer to disk\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtfidf_vectorizer.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m---> 49\u001B[0m     \u001B[43mpkl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtfidf_vectorizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTF-IDF Vectorizer saved to disk.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mPicklingError\u001B[0m: Can't pickle <function <lambda> at 0x000001A0C971E840>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "# Apply feature extraction\n",
    "print('Feature extraction...')\n",
    "features = balanced_df['payload'].apply(lambda x: pd.Series({\n",
    "    'payload_len': calculate_input_length(str(x)),\n",
    "    'alpha': calculate_alphanumeric_ratio(str(x)),\n",
    "    'non_alpha': calculate_special_character_ratio(str(x)),\n",
    "    }))\n",
    "\n",
    "\n",
    "# Concatenate original DF with features\n",
    "balanced_df = pd.concat([balanced_df, features], axis=1)\n",
    "print('Balanced Dataset Shape:', balanced_df.shape)\n",
    "print('Balanced Dataset:')\n",
    "print(balanced_df)\n",
    "\n",
    "# Extracting TF-IDF features from URLs\n",
    "# add to the TFIDF a tokenizer that will split the payload into words by /\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, tokenizer=lambda x: x.split('/'))  # Limiting to top 5000 features\n",
    "\n",
    "# Before fitting the TF-IDF Vectorizer, replace NaN values with empty strings\n",
    "balanced_df['payload'] = balanced_df['payload'].fillna('')\n",
    "\n",
    "# Fit and transform the TF-IDF vectorizer\n",
    "print('Fitting and transforming the TF-IDF Vectorizer to the payloade in the dataset...')\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(balanced_df['payload'])\n",
    "print('TF-IDF Vectorization complete.')\n",
    "\n",
    "# Convert TF-IDF features from a sparse matrix to a dense format and then to an np.ndarray\n",
    "tfidf_dense = np.asarray(tfidf_features.todense())\n",
    "\n",
    "# Define X for numerical features\n",
    "X_numerical = balanced_df.drop(['label', 'payload'], axis=1).values  # Make sure this matches your feature extraction output\n",
    "\n",
    "\n",
    "# Combining TF-IDF features with numerical features\n",
    "X_combined = np.hstack((X_numerical, tfidf_dense))\n",
    "\n",
    "# Define y\n",
    "y = balanced_df['label'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Saving the TF-IDF Vectorizer to disk...')\n",
    "\n",
    "# Save the TF-IDF vectorizer to disk\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pkl.dump(tfidf_vectorizer, f)\n",
    "\n",
    "print('TF-IDF Vectorizer saved to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840aabe4",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b02d59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors training is complete,\n",
      " predictions: \n",
      "[+] \t Classification accuracy :  91.27 %\n",
      "\n",
      "[+] \t Percentage of Anomaly requests in test set :  49.24 %\n",
      "\n",
      "[+] \t Percentage of Normal requests in test set :  50.76 %\n",
      "\n",
      "[+] \t TP :  8481 from  19626\n",
      "    \t TN :  9432 from  19626\n",
      "    \t FP :  530 from  19626\n",
      "    \t FN :  1183 from  19626\n",
      "\n",
      "[+] \t Metrics : \n",
      "\t[-]  Accuracy Score (train_test_split):  91.27 %\n",
      "\t[-]  Accuracy Score (k-fold):  91.29 %\n",
      "\t[-]  Classification Error :  8.73 %\n",
      "\t[-]  Recall :  87.76 %\n",
      "\t[-]  Specificity :  94.68 %\n",
      "\t[-]  False Positive Rate :  5.32 %\n",
      "\t[-]  Precision :  94.12 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an instance of kNN classifier\n",
    "# the number of neighbors (k= 350) \n",
    "knn_model = KNeighborsClassifier(n_neighbors=350) \n",
    "\n",
    "# Train the kNN model (x_train-train_features,y_train-train_labels)\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"K-Nearest Neighbors training is complete,\\n predictions: \")\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = knn_model.predict(X_test)\n",
    "\n",
    "# #Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, predictions)\n",
    "# CMatrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# # Print all evaluation metrics\n",
    "# print(\"Accuracy:\", accuracy,\"Confusion Matrix:\", CMatrix ,\"\\n\",\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# # Extract recall, precision, and F1-score from the classification report\n",
    "# classification_dict = classification_report(y_test, predictions, output_dict=True)\n",
    "# recall = classification_dict['1']['recall']  # Recall for the positive class (malicious)\n",
    "# precision = classification_dict['1']['precision']  # Precision for the positive class (malicious)\n",
    "# f1_score = classification_dict['1']['f1-score']  # F1-score for the positive class (malicious)\n",
    "\n",
    "# # Dump the model to disk\n",
    "# with open('knn_model_counter.pkl', 'wb') as f:\n",
    "#     pkl.dump(knn_model, f)\n",
    "\n",
    "# print('K-Nearest Neighbors model saved to disk.')\n",
    "\n",
    "# print(\"Recall:\", recall)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"F1 score:\", f1_score)\n",
    "\n",
    "\n",
    "#y_test = y_test.values.ravel()\n",
    "\n",
    "print('[+] \\t Classification accuracy : ', \"{:.2f}\".format(metrics.accuracy_score(y_test, predictions) * 100), '%\\n')\n",
    "\n",
    "print('[+] \\t Percentage of Anomaly requests in test set : ', \"{:.2f}\".format(y_test.mean() * 100), '%\\n')\n",
    "print('[+] \\t Percentage of Normal requests in test set : ', \"{:.2f}\".format((1 - y_test.mean()) * 100), '%\\n')\n",
    "\n",
    "confusion = metrics.confusion_matrix(y_test, predictions)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "\n",
    "print('[+] \\t TP : ', TP, 'from ', (TP + TN + FP + FN))\n",
    "print('    \\t TN : ', TN, 'from ', (TP + TN + FP + FN))\n",
    "print('    \\t FP : ', FP, 'from ', (TP + TN + FP + FN))\n",
    "print('    \\t FN : ', FN, 'from ', (TP + TN + FP + FN))\n",
    "\n",
    "print('\\n[+] \\t Metrics : ')\n",
    "print('\\t[-]  Accuracy Score (train_test_split): ', \"{:.2f}\".format(metrics.accuracy_score(y_test, predictions) * 100), '%')\n",
    "print('\\t[-]  Accuracy Score (k-fold): ',\n",
    "      \"{:.2f}\".format(cross_val_score(knn_model, X_train, y_train, cv=100, scoring='accuracy').mean() * 100), '%')\n",
    "\n",
    "print('\\t[-]  Classification Error : ', \"{:.2f}\".format((1 - metrics.accuracy_score(y_test, predictions)) * 100), '%')\n",
    "print('\\t[-]  Recall : ', \"{:.2f}\".format(metrics.recall_score(y_test, predictions) * 100), '%')\n",
    "specificity = TN / (TN + FP)\n",
    "print('\\t[-]  Specificity : ', \"{:.2f}\".format(specificity * 100), '%')\n",
    "false_positive_rate = FP / float(TN + FP)\n",
    "print('\\t[-]  False Positive Rate : ', \"{:.2f}\".format(false_positive_rate * 100), '%')\n",
    "precision = TP / float(TP + FP)\n",
    "print('\\t[-]  Precision : ', \"{:.2f}\".format(precision * 100), '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2f42c",
   "metadata": {},
   "source": [
    "## TensorFlow neural network, with feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4392f",
   "metadata": {},
   "source": [
    "A deep learning model using a Dense Neural Network (DNN), also known as a Fully Connected Network. This type of neural network architecture is characterized by its layers of neurons where each neuron in one layer is connected to all neurons in the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e58bf6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shaha\\PycharmProjects\\CMD_final_project\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shaha\\PycharmProjects\\CMD_final_project\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\shaha\\PycharmProjects\\CMD_final_project\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shaha\\PycharmProjects\\CMD_final_project\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1963/1963 [==============================] - 5s 2ms/step - loss: 0.2323 - accuracy: 0.9163 - recall: 0.8931 - precision: 0.9334 - val_loss: 0.1977 - val_accuracy: 0.9267 - val_recall: 0.8941 - val_precision: 0.9531\n",
      "Epoch 2/10\n",
      "1963/1963 [==============================] - 3s 2ms/step - loss: 0.1822 - accuracy: 0.9343 - recall: 0.9108 - precision: 0.9531 - val_loss: 0.1695 - val_accuracy: 0.9401 - val_recall: 0.9195 - val_precision: 0.9563\n",
      "Epoch 3/10\n",
      "1963/1963 [==============================] - 3s 2ms/step - loss: 0.1664 - accuracy: 0.9415 - recall: 0.9118 - precision: 0.9670 - val_loss: 0.1634 - val_accuracy: 0.9425 - val_recall: 0.9118 - val_precision: 0.9687\n",
      "Epoch 4/10\n",
      "1963/1963 [==============================] - 3s 2ms/step - loss: 0.1606 - accuracy: 0.9440 - recall: 0.9121 - precision: 0.9719 - val_loss: 0.1620 - val_accuracy: 0.9439 - val_recall: 0.9130 - val_precision: 0.9707\n",
      "Epoch 5/10\n",
      "1963/1963 [==============================] - 3s 2ms/step - loss: 0.1573 - accuracy: 0.9454 - recall: 0.9126 - precision: 0.9743 - val_loss: 0.1613 - val_accuracy: 0.9440 - val_recall: 0.9109 - val_precision: 0.9729\n",
      "Epoch 6/10\n",
      "1963/1963 [==============================] - 3s 2ms/step - loss: 0.1571 - accuracy: 0.9456 - recall: 0.9122 - precision: 0.9753 - val_loss: 0.1623 - val_accuracy: 0.9425 - val_recall: 0.9047 - val_precision: 0.9760\n",
      "Epoch 7/10\n",
      "1963/1963 [==============================] - 3s 2ms/step - loss: 0.1530 - accuracy: 0.9471 - recall: 0.9133 - precision: 0.9774 - val_loss: 0.1665 - val_accuracy: 0.9443 - val_recall: 0.9250 - val_precision: 0.9597\n",
      "Epoch 8/10\n",
      "1963/1963 [==============================] - 3s 2ms/step - loss: 0.1551 - accuracy: 0.9464 - recall: 0.9125 - precision: 0.9765 - val_loss: 0.1570 - val_accuracy: 0.9455 - val_recall: 0.9150 - val_precision: 0.9721\n",
      "Epoch 9/10\n",
      "1963/1963 [==============================] - 3s 2ms/step - loss: 0.1516 - accuracy: 0.9478 - recall: 0.9143 - precision: 0.9777 - val_loss: 0.1589 - val_accuracy: 0.9457 - val_recall: 0.9197 - val_precision: 0.9677\n",
      "Epoch 10/10\n",
      "1963/1963 [==============================] - 3s 2ms/step - loss: 0.1509 - accuracy: 0.9476 - recall: 0.9142 - precision: 0.9774 - val_loss: 0.1710 - val_accuracy: 0.9429 - val_recall: 0.9234 - val_precision: 0.9582\n",
      "Test Accuracy: 0.942\n",
      "Recall: 0.920\n",
      "Precision: 0.961\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_features = X_train.shape[1]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Recall', 'Precision'])\n",
    "# fit the model\n",
    "\n",
    "# use asarray to convert the dataframe to array\n",
    "X_train_1 = np.asarray(X_train)\n",
    "y_train_1 = np.asarray(y_train)\n",
    "\n",
    "model.fit(X_train_1, y_train_1, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "# evaluate the model\n",
    "X_test_1 = np.asarray(X_test)\n",
    "y_test_1 = np.asarray(y_test)\n",
    "\n",
    "loss, accuracy, recall, precision = model.evaluate(X_test_1, y_test_1, verbose=0)\n",
    "# print the accuracy in a different way\n",
    "print('Test Accuracy: %.3f' % accuracy)\n",
    "print('Recall: %.3f' % recall)\n",
    "print('Precision: %.3f' % precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ensemble learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy: 0.9503210027514521\n",
      "Ensemble Model Confusion Matrix: [[9726  236]\n",
      " [ 739 8925]]\n",
      "Ensemble Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95      9962\n",
      "           1       0.97      0.92      0.95      9664\n",
      "\n",
      "    accuracy                           0.95     19626\n",
      "   macro avg       0.95      0.95      0.95     19626\n",
      "weighted avg       0.95      0.95      0.95     19626\n",
      "\n",
      "Ensemble model saved to disk.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "# Initialize individual models\n",
    "log_clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "dt_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "svm_clf = GaussianNB() # Enable probability for soft voting\n",
    "#svm_clf = ExtraTreesClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "\n",
    "# Create ensemble model using soft voting\n",
    "ensemble_clf = VotingClassifier(estimators=[\n",
    "    ('lr', log_clf),\n",
    "    ('dt', dt_clf),\n",
    "    ('svc', dt_clf),\n",
    "    #('knn', knn_model)  # Add your kNN model\n",
    "    ], voting='soft')\n",
    "\n",
    "ensemble_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "ensemble_predictions = ensemble_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "ensemble_CMatrix = confusion_matrix(y_test, ensemble_predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Ensemble Model Accuracy:\", ensemble_accuracy)\n",
    "print(\"Ensemble Model Confusion Matrix:\", ensemble_CMatrix)\n",
    "print(\"Ensemble Classification Report:\\n\", classification_report(y_test, ensemble_predictions))\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the ensemble model to disk\n",
    "with open('ensemble_model.pkl', 'wb') as f:\n",
    "    pickle.dump(ensemble_clf, f)\n",
    "\n",
    "print('Ensemble model saved to disk.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
